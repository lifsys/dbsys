{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, Optional, Union, List, Callable\n",
    "from sqlalchemy import create_engine, text, MetaData, Table, Column, exc as sa_exc\n",
    "from sqlalchemy.engine import Engine\n",
    "from pathlib import Path\n",
    "import redis\n",
    "from urllib.parse import urlparse\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from redis.exceptions import ConnectionError, TimeoutError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseError(Exception):\n",
    "    \"\"\"Base exception for database operations.\"\"\"\n",
    "    pass\n",
    "\n",
    "class TableNotFoundError(DatabaseError):\n",
    "    \"\"\"Raised when a specified table is not found in the database.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ColumnNotFoundError(DatabaseError):\n",
    "    \"\"\"Raised when a specified column is not found in the table.\"\"\"\n",
    "    pass\n",
    "\n",
    "class InvalidOperationError(DatabaseError):\n",
    "    \"\"\"Raised when an invalid operation is attempted.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseManager:\n",
    "    def __init__(self, connection_string: str):\n",
    "        self._connection_string = connection_string\n",
    "        self._table_name = None\n",
    "        self._pubsub = None\n",
    "        self._subscriber_thread = None\n",
    "        self._close_flag = threading.Event()\n",
    "        self._close_message = None\n",
    "        self._message_history = defaultdict(list)\n",
    "\n",
    "        parsed_url = urlparse(connection_string)\n",
    "        if parsed_url.scheme == 'redis':\n",
    "            self._base = 'redis'\n",
    "            self._redis_client = redis.Redis.from_url(connection_string)\n",
    "            self._engine = None\n",
    "        elif parsed_url.scheme in ['postgresql', 'mysql', 'sqlite']:\n",
    "            self._base = 'sql'\n",
    "            self._engine = create_engine(connection_string)\n",
    "            self._redis_client = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported database type: {parsed_url.scheme}\")\n",
    "\n",
    "    def table(self, table_name: str) -> 'DatabaseManager':\n",
    "        self._table_name = f'\"{table_name}\"'\n",
    "        return self\n",
    "\n",
    "    def read(self) -> pd.DataFrame:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            try:\n",
    "                query = f\"SELECT * FROM {self._table_name}\"\n",
    "                return pd.read_sql_query(query, self._engine)\n",
    "            except ValueError as ve:\n",
    "                raise DatabaseError(f\"Error reading table: {str(ve)}\")\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Database operation failed: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Read operation not implemented for Redis\")\n",
    "\n",
    "    def write(self, data: pd.DataFrame) -> bool:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            try:\n",
    "                data.to_sql(self._table_name, self._engine, if_exists='replace', index=False)\n",
    "                return True\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Failed to write to table: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Write operation not implemented for Redis\")\n",
    "\n",
    "    def create(self, data: pd.DataFrame) -> bool:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            try:\n",
    "                data.to_sql(self._table_name, self._engine, if_exists='fail', index=False)\n",
    "                return True\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Failed to create table: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Create operation not implemented for Redis\")\n",
    "\n",
    "    def delete_table(self) -> bool:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            try:\n",
    "                with self._engine.connect() as connection:\n",
    "                    connection.execute(text(f\"DROP TABLE IF EXISTS {self._table_name}\"))\n",
    "                return True\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Failed to delete table: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Delete table operation not implemented for Redis\")\n",
    "\n",
    "    def delete_row(self, row_identifier: Dict[str, Any]) -> int:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            if not row_identifier:\n",
    "                raise ValueError(\"Row identifier must be provided for delete row operation\")\n",
    "            \n",
    "            try:\n",
    "                conditions = []\n",
    "                for key, value in row_identifier.items():\n",
    "                    if value is None:\n",
    "                        conditions.append(f'\"{key}\" IS NULL')\n",
    "                    else:\n",
    "                        conditions.append(f'\"{key}\" = :{key}')\n",
    "                \n",
    "                where_clause = \" AND \".join(conditions)\n",
    "                query = f\"DELETE FROM {self._table_name} WHERE {where_clause}\"\n",
    "                \n",
    "                with self._engine.connect() as connection:\n",
    "                    result = connection.execute(\n",
    "                        text(query),\n",
    "                        {k: v for k, v in row_identifier.items() if v is not None}\n",
    "                    )\n",
    "                    connection.commit()\n",
    "                    return result.rowcount\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Failed to delete row: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Delete row operation not implemented for Redis\")\n",
    "\n",
    "    def search(self, conditions: Union[Dict[str, Any], str], limit: Optional[int] = None, case_sensitive: bool = False) -> pd.DataFrame:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "            \n",
    "            try:\n",
    "                if isinstance(conditions, dict):\n",
    "                    if not conditions:\n",
    "                        raise ValueError(\"Search conditions dictionary cannot be empty\")\n",
    "                    where_clauses = []\n",
    "                    search_conditions = {}\n",
    "                    for i, (col, val) in enumerate(conditions.items()):\n",
    "                        if val is None:\n",
    "                            raise ValueError(f\"Search value for column '{col}' cannot be None\")\n",
    "                        param_name = f\"param_{i}\"\n",
    "                        if case_sensitive:\n",
    "                            where_clauses.append(f'\"{col}\" LIKE :{param_name}')\n",
    "                        else:\n",
    "                            where_clauses.append(f'LOWER(\"{col}\"::text) LIKE LOWER(:{param_name})')\n",
    "                        search_conditions[param_name] = f\"%{val}%\"\n",
    "                    where_clause = \" AND \".join(where_clauses)\n",
    "                elif isinstance(conditions, str):\n",
    "                    if not conditions.strip():\n",
    "                        raise ValueError(\"Search string cannot be empty\")\n",
    "                    where_clause = conditions\n",
    "                    search_conditions = {}\n",
    "                else:\n",
    "                    raise ValueError(\"conditions must be either a non-empty dictionary or a non-empty string\")\n",
    "\n",
    "                query = f\"SELECT * FROM {self._table_name} WHERE {where_clause}\"\n",
    "                if limit is not None:\n",
    "                    query += f\" LIMIT {limit}\"\n",
    "                \n",
    "                logger.debug(f\"Executing SQL: {query}\")\n",
    "                logger.debug(f\"With parameters: {search_conditions}\")\n",
    "                \n",
    "                with self._engine.connect() as connection:\n",
    "                    result = connection.execute(text(query), search_conditions)\n",
    "                    return pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Search operation failed: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Search operation not implemented for Redis\")\n",
    "\n",
    "    def backup(self, file_path: str, columns: Optional[List[str]] = None) -> bool:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "\n",
    "            try:\n",
    "                data = self.read()\n",
    "\n",
    "                if columns:\n",
    "                    missing_columns = set(columns) - set(data.columns)\n",
    "                    if missing_columns:\n",
    "                        raise ValueError(f\"Columns not found in table: {', '.join(missing_columns)}\")\n",
    "                    data_to_backup = data[columns]\n",
    "                else:\n",
    "                    data_to_backup = data\n",
    "\n",
    "                json_data = data_to_backup.to_json(orient='records', date_format='iso')\n",
    "\n",
    "                Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with open(file_path, 'w') as f:\n",
    "                    f.write(json_data)\n",
    "\n",
    "                logger.info(f\"{self._table_name}: Backup created successfully at {file_path}\")\n",
    "                return True\n",
    "\n",
    "            except (sa_exc.SQLAlchemyError, IOError) as e:\n",
    "                raise DatabaseError(f\"Failed to create backup: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Backup operation not implemented for Redis\")\n",
    "\n",
    "    def restore(self, file_path: str, mode: str = 'replace') -> bool:\n",
    "        if self._base == 'sql':\n",
    "            if not self._table_name:\n",
    "                raise ValueError(\"Table name not set. Use .table() first.\")\n",
    "\n",
    "            if not Path(file_path).exists():\n",
    "                raise ValueError(f\"File not found: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "\n",
    "                df = pd.DataFrame(json_data)\n",
    "\n",
    "                if mode == 'replace':\n",
    "                    return self.write(df)\n",
    "                elif mode == 'append':\n",
    "                    df.to_sql(self._table_name, self._engine, if_exists='append', index=False)\n",
    "                    return True\n",
    "                elif mode == 'upsert':\n",
    "                    metadata = MetaData()\n",
    "                    table = Table(self._table_name, metadata, autoload_with=self._engine)\n",
    "                    pk_columns = [key.name for key in table.primary_key]\n",
    "\n",
    "                    if not pk_columns:\n",
    "                        raise ValueError(\"Cannot perform upsert without primary key\")\n",
    "\n",
    "                    for _, row in df.iterrows():\n",
    "                        query = f\"\"\"\n",
    "                        INSERT INTO {self._table_name} ({', '.join(df.columns)})\n",
    "                        VALUES ({', '.join([':' + col for col in df.columns])})\n",
    "                        ON CONFLICT ({', '.join(pk_columns)})\n",
    "                        DO UPDATE SET {', '.join([f\"{col} = excluded.{col}\" for col in df.columns if col not in pk_columns])}\n",
    "                        \"\"\"\n",
    "                        with self._engine.connect() as conn:\n",
    "                            conn.execute(text(query), row.to_dict())\n",
    "                            conn.commit()\n",
    "                    return True\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid mode. Use 'replace', 'append', or 'upsert'.\")\n",
    "\n",
    "            except (sa_exc.SQLAlchemyError, IOError, json.JSONDecodeError) as e:\n",
    "                raise DatabaseError(f\"Failed to restore from backup: {str(e)}\")\n",
    "        elif self._base == 'redis':\n",
    "            raise NotImplementedError(\"Restore operation not implemented for Redis\")\n",
    "\n",
    "    def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> Union[pd.DataFrame, int]:\n",
    "        if self._base == 'sql':\n",
    "            try:\n",
    "                with self._engine.connect() as connection:\n",
    "                    result = connection.execute(text(query), params or {})\n",
    "                    if query.strip().upper().startswith('SELECT'):\n",
    "                        return pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "                    else:\n",
    "                        connection.commit()\n",
    "                        return result.rowcount\n",
    "            except sa_exc.SQLAlchemyError as e:\n",
    "                raise DatabaseError(f\"Query execution failed: {str(e)}\")\n",
    "        else:\n",
    "            raise ValueError(\"execute_query method is only available for SQL databases\")\n",
    "\n",
    "    # Redis-specific methods remain largely unchanged\n",
    "    def pub(self, message: str, channel: str) -> int:\n",
    "        return self._redis_client.publish(channel, message)\n",
    "\n",
    "    def sub(self, channel: str, handler: Optional[Callable[[str, str], None]] = None, exiton: str = \"\") -> 'DatabaseManager':\n",
    "        if self._pubsub is not None:\n",
    "            self.unsub()  # Unsubscribe from any existing subscriptions\n",
    "\n",
    "        self._pubsub = self._redis_client.pubsub()\n",
    "        self._close_message = exiton\n",
    "\n",
    "        def wrapped_handler(message):\n",
    "            if message['type'] == 'message':\n",
    "                data = message['data'].decode('utf-8')\n",
    "                if data == exiton:\n",
    "                    logger.info(f\"Received close message: {exiton}\")\n",
    "                    self._close_flag.set()\n",
    "                elif handler:\n",
    "                    handler(channel, data)\n",
    "                else:\n",
    "                    logger.info(f\"Received message on channel {channel}: {data}\")\n",
    "\n",
    "        self._pubsub.subscribe(**{channel: wrapped_handler})\n",
    "        \n",
    "        self._subscriber_thread = threading.Thread(target=self._message_handler_loop, daemon=True)\n",
    "        self._subscriber_thread.start()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def pubsub(self, pub_message: str, pub_channel: str, sub_channel: str, \n",
    "               handler: Optional[Callable[[str, str], None]] = None, \n",
    "               exiton: str = \"CLOSE\", \n",
    "               wait: Optional[int] = None,\n",
    "               max_retries: int = 3,\n",
    "               retry_delay: float = 1.0) -> 'DatabaseManager':\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self._close_flag.clear()\n",
    "                \n",
    "                def wrapped_handler(channel, message):\n",
    "                    if hasattr(self, '_execon_trigger') and message == self._execon_trigger:\n",
    "                        self._execon_func()\n",
    "                    if handler:\n",
    "                        handler(channel, message)\n",
    "                    else:\n",
    "                        self.message_handler(channel, message)\n",
    "\n",
    "                self.sub(sub_channel, wrapped_handler, exiton)\n",
    "                self.pub(pub_message, pub_channel)\n",
    "\n",
    "                try:\n",
    "                    if wait is not None:\n",
    "                        self._close_flag.wait(timeout=wait)\n",
    "                    else:\n",
    "                        self._close_flag.wait()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error while waiting for close flag: {str(e)}\")\n",
    "                finally:\n",
    "                    self._close_flag.set()  # Ensure the flag is set even if an exception occurs\n",
    "\n",
    "                break  # If we get here, the operation was successful\n",
    "            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError) as e:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(\"Max retries reached. Operation failed.\")\n",
    "                    raise\n",
    "                time.sleep(retry_delay)\n",
    "            finally:\n",
    "                self.unsub(sub_channel)\n",
    "                if self._pubsub:\n",
    "                    try:\n",
    "                        self._pubsub.close()\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error closing pubsub connection: {str(e)}\")\n",
    "                \n",
    "                # Ensure the subscriber thread is terminated\n",
    "                if self._subscriber_thread and self._subscriber_thread.is_alive():\n",
    "                    self._subscriber_thread.join(timeout=2)\n",
    "                    if self._subscriber_thread.is_alive():\n",
    "                        logger.warning(\"Subscriber thread did not terminate within the timeout period.\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __del__(self):\n",
    "        self.unsub()\n",
    "        if hasattr(self, '_redis_client') and self._redis_client is not None:\n",
    "            self._redis_client.close()\n",
    "\n",
    "    def unsub(self, channel: Optional[str] = None) -> 'DatabaseManager':\n",
    "        # Set the close flag to stop the message handler loop\n",
    "        self._close_flag.set()\n",
    "\n",
    "        # Wait for the subscriber thread to finish\n",
    "        if self._subscriber_thread:\n",
    "            self._subscriber_thread.join(timeout=2)  # Increased timeout for thread to finish\n",
    "            if self._subscriber_thread.is_alive():\n",
    "                logger.warning(\"Subscriber thread did not terminate within the timeout period.\")\n",
    "            self._subscriber_thread = None\n",
    "\n",
    "        if self._pubsub is not None:\n",
    "            try:\n",
    "                # Unsubscribe from specific channel or all channels\n",
    "                if channel:\n",
    "                    self._pubsub.unsubscribe(channel)\n",
    "                else:\n",
    "                    self._pubsub.unsubscribe()\n",
    "                    self._pubsub.punsubscribe()  # Unsubscribe from all pattern subscriptions as well\n",
    "                \n",
    "                # Close the pubsub connection\n",
    "                self._pubsub.close()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error during unsubscribe: {str(e)}\")\n",
    "            finally:\n",
    "                self._pubsub = None\n",
    "\n",
    "        # Clear the close flag\n",
    "        self._close_flag.clear()\n",
    "\n",
    "        # Clear message history for the unsubscribed channel(s)\n",
    "        if channel:\n",
    "            self._message_history.pop(channel, None)\n",
    "        else:\n",
    "            self._message_history.clear()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_stored_messages(self, channel: str) -> List[str]:\n",
    "        return self._message_history.get(channel, [])\n",
    "\n",
    "    def clear_stored_messages(self, channel: Optional[str] = None) -> 'DatabaseManager':\n",
    "        if channel:\n",
    "            self._message_history[channel] = []\n",
    "        else:\n",
    "            self._message_history.clear()\n",
    "        return self\n",
    "\n",
    "    def _message_handler_wrapper(self, user_handler: Optional[Callable[[str, str], None]], exiton: str) -> Callable[[dict], None]:\n",
    "        def wrapper(message):\n",
    "            if message['type'] == 'message':\n",
    "                channel = message['channel'].decode('utf-8')\n",
    "                data = message['data'].decode('utf-8')\n",
    "                self._message_history[channel].append(data)\n",
    "                if data == exiton:\n",
    "                    logger.info(f\"Received close message: {exiton}\")\n",
    "                    self._close_flag.set()\n",
    "                else:\n",
    "                    if user_handler:\n",
    "                        user_handler(channel, data)\n",
    "                    else:\n",
    "                        self.message_handler(channel, data)\n",
    "        return wrapper\n",
    "\n",
    "    def _message_handler_loop(self):\n",
    "        try:\n",
    "            while not self._close_flag.is_set():\n",
    "                message = self._pubsub.get_message(timeout=1)\n",
    "                if message:\n",
    "                    if message['type'] == 'message':\n",
    "                        channel = message['channel'].decode('utf-8')\n",
    "                        data = message['data'].decode('utf-8')\n",
    "                        if data == self._close_message:\n",
    "                            logger.info(f\"Received close message: {self._close_message}\")\n",
    "                            self._close_flag.set()\n",
    "                            break\n",
    "                        else:\n",
    "                            self.message_handler(channel, data)\n",
    "        except redis.exceptions.ConnectionError as e:\n",
    "            logger.warning(f\"Redis connection closed: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in message handler loop: {str(e)}\")\n",
    "        finally:\n",
    "            logger.info(\"Message handler loop ended\")\n",
    "            if self._pubsub:\n",
    "                try:\n",
    "                    self._pubsub.close()\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error closing pubsub connection: {str(e)}\")\n",
    "\n",
    "    def message_handler(self, channel: str, message: str):\n",
    "        \"\"\"\n",
    "        Default message handler for received messages.\n",
    "\n",
    "        Args:\n",
    "            channel (str): The channel on which the message was received.\n",
    "            message (str): The received message.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Received message on channel {channel}: {message}\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        base_type = self._base\n",
    "        table_name = self._table_name or \"Not set\"\n",
    "        redis_info = f\", subscribed channels: {list(self._pubsub.channels) if self._pubsub else None}\" if self._base == 'redis' else \"\"\n",
    "        return f\"DatabaseManager(base={base_type}, table={table_name}{redis_info})\"\n",
    "\n",
    "    def stream(self, stream_name: str) -> 'DatabaseManager':\n",
    "        if self._base != 'redis':\n",
    "            raise ValueError(\"Stream operations are only available for Redis\")\n",
    "        self._stream_name = stream_name\n",
    "        return self\n",
    "\n",
    "    def stream_add(self, data: Dict[str, str]) -> 'DatabaseManager':\n",
    "        if not hasattr(self, '_stream_name'):\n",
    "            raise ValueError(\"Stream name not set. Use .stream() first.\")\n",
    "        self._redis_client.xadd(self._stream_name, data)\n",
    "        return self\n",
    "\n",
    "    def stream_read(self, count: int = 100, block: int = None) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        if not hasattr(self, '_stream_name'):\n",
    "            raise ValueError(\"Stream name not set. Use .stream() first.\")\n",
    "        raw_data = self._redis_client.xread({self._stream_name: '0'}, count=count, block=block)\n",
    "        \n",
    "        # Process the raw data into a more user-friendly dictionary format\n",
    "        result = {}\n",
    "        for stream, messages in raw_data:\n",
    "            stream_name = stream.decode('utf-8')\n",
    "            result[stream_name] = []\n",
    "            for message_id, message_data in messages:\n",
    "                entry = {\n",
    "                    'id': message_id.decode('utf-8'),\n",
    "                    'data': {k.decode('utf-8'): v.decode('utf-8') for k, v in message_data.items()}\n",
    "                }\n",
    "                result[stream_name].append(entry)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def json(self, key: str) -> 'DatabaseManager':\n",
    "        if self._base != 'redis':\n",
    "            raise ValueError(\"JSON operations are only available for Redis\")\n",
    "        self._json_key = key\n",
    "        return self\n",
    "\n",
    "    def json_set(self, data: Dict[str, Any]) -> 'DatabaseManager':\n",
    "        if not hasattr(self, '_json_key'):\n",
    "            raise ValueError(\"JSON key not set. Use .json() first.\")\n",
    "        self._redis_client.json().set(self._json_key, '.', json.dumps(data))\n",
    "        return self\n",
    "\n",
    "    def json_get(self) -> Dict[str, Any]:\n",
    "        if not hasattr(self, '_json_key'):\n",
    "            raise ValueError(\"JSON key not set. Use .json() first.\")\n",
    "        result = self._redis_client.json().get(self._json_key)\n",
    "        return json.loads(result) if result else None\n",
    "\n",
    "    def list(self, key: str) -> 'DatabaseManager':\n",
    "        if self._base != 'redis':\n",
    "            raise ValueError(\"List operations are only available for Redis\")\n",
    "        self._list_key = key\n",
    "        return self\n",
    "\n",
    "    def list_push(self, *values) -> 'DatabaseManager':\n",
    "        if not hasattr(self, '_list_key'):\n",
    "            raise ValueError(\"List key not set. Use .list() first.\")\n",
    "        self._redis_client.rpush(self._list_key, *values)\n",
    "        return self\n",
    "\n",
    "    def list_get(self, start: int = 0, end: int = -1) -> List[str]:\n",
    "        if not hasattr(self, '_list_key'):\n",
    "            raise ValueError(\"List key not set. Use .list() first.\")\n",
    "        return self._redis_client.lrange(self._list_key, start, end)\n",
    "\n",
    "    def string(self, key: str) -> 'DatabaseManager':\n",
    "        if self._base != 'redis':\n",
    "            raise ValueError(\"String operations are only available for Redis\")\n",
    "        self._string_key = key\n",
    "        return self\n",
    "\n",
    "    def string_set(self, value: str) -> 'DatabaseManager':\n",
    "        if not hasattr(self, '_string_key'):\n",
    "            raise ValueError(\"String key not set. Use .string() first.\")\n",
    "        self._redis_client.set(self._string_key, value)\n",
    "        return self\n",
    "\n",
    "    def string_get(self) -> Optional[str]:\n",
    "        if not hasattr(self, '_string_key'):\n",
    "            raise ValueError(\"String key not set. Use .string() first.\")\n",
    "        return self._redis_client.get(self._string_key)\n",
    "\n",
    "    def execon(self, trigger: str, func: Callable[..., Any], *args, **kwargs) -> 'DatabaseManager':\n",
    "        \"\"\"\n",
    "        Set up a function to be executed when a specific message is received during pubsub.\n",
    "\n",
    "        Args:\n",
    "            trigger (str): The message that triggers the function execution.\n",
    "            func (Callable[..., Any]): The function to be executed when the trigger is received.\n",
    "            *args: Positional arguments to pass to the function.\n",
    "            **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "        Returns:\n",
    "            DatabaseManager: The current instance for method chaining.\n",
    "        \"\"\"\n",
    "        self._execon_trigger = trigger\n",
    "        self._execon_func = lambda: func(*args, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locksys import Locksys\n",
    "DBCONNECT = Locksys().item(\"lifsysdb\").key(\"lifsysdb\").results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name                    position_applied         location  \\\n",
      "0          Jason Clark    Logistics Analyst (Intermediate)   Huntsville, AL   \n",
      "1  John Bradley Hiland        Logistics Analyst (Advanced)   Huntsville, AL   \n",
      "2      Thaddius Pruitt           Acquisition Analyst (SME)   Huntsville, AL   \n",
      "3      Dawn M Harbison        Logistics Analyst (Advanced)  Fort Greely, AK   \n",
      "4         David Flippo  Earned Value Program Analyst (ADV)     Dahlgren, VA   \n",
      "\n",
      "        salary_requirement                phone                     email  \\\n",
      "0                     None  {\"+1 256-822-9307\"}        clark127@gmail.com   \n",
      "1                           {\"+1 256-682-2711\"}        jbhiland@gmail.com   \n",
      "2                           {\"+1 256-933-0702\"}  thaddiuspruitt@gmail.com   \n",
      "3  $110,000.00-$130,000.00  {\"+1 775-781-8549\"}          dmcas2@yahoo.com   \n",
      "4                           {\"+1 540-848-2256\"}  flippoistheman@gmail.com   \n",
      "\n",
      "                                     address  \\\n",
      "0                          Decatur, AL 35601   \n",
      "1  2121 Villaret Drive, Huntsville, AL 35803   \n",
      "2                       New Market, AL 35761   \n",
      "3   2586 Jannel St, Delta Junction, AK 99737   \n",
      "4                      King George, VA 22485   \n",
      "\n",
      "                                           education  \\\n",
      "0                          [\"Associates of Science\"]   \n",
      "1  [\"Bachelor of Science in Commerce & Business A...   \n",
      "2  [\"Bachelor's degree in Business Administration...   \n",
      "3  [\"Bachelor in English from American Military U...   \n",
      "4  [\"Bachelor's degree in Contracts and Acquisiti...   \n",
      "\n",
      "                                      certifications         clearance  ...  \\\n",
      "0  [\"Professional Development Program Management ...        [\"Secret\"]  ...   \n",
      "1  [\"Hazmat Certified\", \"Counterfeit Parts Certif...  [\"Public Trust\"]  ...   \n",
      "2                                                 []          [\"None\"]  ...   \n",
      "3  [\"AWSC (Arctic Warrior Supply Course)\", \"GCSS-...        [\"Secret\"]  ...   \n",
      "4                                                 []        [\"Secret\"]  ...   \n",
      "\n",
      "       Last   Middle SUFFIX  \\\n",
      "0     Clark                   \n",
      "1    Hiland  Bradley          \n",
      "2    Pruitt                   \n",
      "3  Harbison        M          \n",
      "4    Flippo                   \n",
      "\n",
      "                                          assessment                LcatKey  \\\n",
      "0  [{\"initial_impression\": {\"resume_overview\": \"T...  HQ085822C0004.LAI.RSA   \n",
      "1  [{\"initial_impression\": {\"resume_overview\": \"T...  HQ085822C0004.LAA.RSA   \n",
      "2  [{\"initial_impression\": {\"resume_overview\": \"T...  HQ085824C0002.AAS.HSV   \n",
      "3  [{\"initial_impression\": {\"resume_overview\": \"T...  HQ085822C0004.LAA.HIK   \n",
      "4  [{\"initial_impression\": {\"resume_overview\": \"T...  HQ085824C0002.EAA.DLG   \n",
      "\n",
      "                                          LcatMatch  \\\n",
      "0                    Logistics Intermediate Analyst   \n",
      "1                        Logistics Advanced Analyst   \n",
      "2  Acquisition and Program Management Analyst (SME)   \n",
      "3                        Logistics Advanced Analyst   \n",
      "4          Earned Value Program Analyst (Advanced)    \n",
      "\n",
      "                                                  id    Applied score Suffix  \n",
      "0  http://jobs.localjobnetwork.com/app/34370667/8...  6/11/2024     4   None  \n",
      "1  http://jobs.localjobnetwork.com/app/34483934/3...  7/31/2024     2   None  \n",
      "2  http://jobs.localjobnetwork.com/app/34370361/8...  6/11/2024     1   None  \n",
      "3  http://jobs.localjobnetwork.com/app/34484991/8...  7/31/2024     2   None  \n",
      "4  http://jobs.localjobnetwork.com/app/34389538/6...  6/21/2024     1   None  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "db_manager = DatabaseManager(DBCONNECT)\n",
    "query = 'SELECT * FROM \"applicants\" LIMIT 5'\n",
    "result = db_manager.execute_query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LCAT</th>\n",
       "      <th>LCode</th>\n",
       "      <th>LoCode</th>\n",
       "      <th>Contract</th>\n",
       "      <th>LcatKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Strategic Planning Lead</td>\n",
       "      <td>SPS</td>\n",
       "      <td>HSV</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.SPS.HSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strategic Planning Analyst (Intermediate)</td>\n",
       "      <td>SPI</td>\n",
       "      <td>COS</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.SPI.COS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strategic Planning Analyst (Intermediate)</td>\n",
       "      <td>SPI</td>\n",
       "      <td>DLG</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.SPI.DLG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strategic Planning Analyst (Intermediate)</td>\n",
       "      <td>SPI</td>\n",
       "      <td>FBV</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.SPI.FBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategic Planning Analyst (Intermediate)</td>\n",
       "      <td>SPI</td>\n",
       "      <td>HSV</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.SPI.HSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Acquisition and Program Management Analyst (Ad...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>COS</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.AAA.COS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Acquisition and Program Management Analyst (Ad...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>DLG</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.AAA.DLG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Acquisition and Program Management Analyst (Ad...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>HAFB</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.AAA.HAFB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Acquisition and Program Management Analyst (Ad...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>HSV</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.AAA.HSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Acquisition and Program Management Analyst (Ad...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>KAFB</td>\n",
       "      <td>HQ085824C0002</td>\n",
       "      <td>HQ085824C0002.AAA.KAFB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  LCAT LCode LoCode  \\\n",
       "0                              Strategic Planning Lead   SPS    HSV   \n",
       "1            Strategic Planning Analyst (Intermediate)   SPI    COS   \n",
       "2            Strategic Planning Analyst (Intermediate)   SPI    DLG   \n",
       "3            Strategic Planning Analyst (Intermediate)   SPI    FBV   \n",
       "4            Strategic Planning Analyst (Intermediate)   SPI    HSV   \n",
       "..                                                 ...   ...    ...   \n",
       "133  Acquisition and Program Management Analyst (Ad...   AAA    COS   \n",
       "134  Acquisition and Program Management Analyst (Ad...   AAA    DLG   \n",
       "135  Acquisition and Program Management Analyst (Ad...   AAA   HAFB   \n",
       "136  Acquisition and Program Management Analyst (Ad...   AAA    HSV   \n",
       "137  Acquisition and Program Management Analyst (Ad...   AAA   KAFB   \n",
       "\n",
       "          Contract                 LcatKey  \n",
       "0    HQ085824C0002   HQ085824C0002.SPS.HSV  \n",
       "1    HQ085824C0002   HQ085824C0002.SPI.COS  \n",
       "2    HQ085824C0002   HQ085824C0002.SPI.DLG  \n",
       "3    HQ085824C0002   HQ085824C0002.SPI.FBV  \n",
       "4    HQ085824C0002   HQ085824C0002.SPI.HSV  \n",
       "..             ...                     ...  \n",
       "133  HQ085824C0002   HQ085824C0002.AAA.COS  \n",
       "134  HQ085824C0002   HQ085824C0002.AAA.DLG  \n",
       "135  HQ085824C0002  HQ085824C0002.AAA.HAFB  \n",
       "136  HQ085824C0002   HQ085824C0002.AAA.HSV  \n",
       "137  HQ085824C0002  HQ085824C0002.AAA.KAFB  \n",
       "\n",
       "[138 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_manager = DatabaseManager(DBCONNECT)\n",
    "result = db_manager.table(\"LCAT\").read()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
